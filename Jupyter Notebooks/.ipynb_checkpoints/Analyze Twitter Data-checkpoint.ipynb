{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import json\n",
    "import requests\n",
    "import textblob\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras.preprocessing.text as kpt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import load_model\n",
    "from arcgis.gis import GIS\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from textblob.np_extractors import ConllExtractor\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = r'C:\\Users\\jame9353\\Documents\\GitHub\\NLP-Engine-Integration\\Demo Data\\Baltimore Riots Tweets\\baltimore_twitter.csv'\n",
    "model = load_model('models/Twitter_SA_Model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def netowlCurl(inFile, outPath, outExtension):\n",
    "    headers = {\n",
    "    'accept': 'application/rdf+xml',\n",
    "    'Authorization': 'netowl ff5e6185-5d63-459b-9765-4ebb905affc8',\n",
    "    }\n",
    "    \n",
    "    \n",
    "    headers['Content-Type'] = 'text/plain'\n",
    "        \n",
    "    params = (\n",
    "        ('language', 'english'),\n",
    "    )\n",
    "    \n",
    "    data = open(inFile, 'rb').read()\n",
    "    response = requests.post('https://api.netowl.com/api/v2/_process', headers=headers, params=params, data=data, verify=False)\n",
    "    r = response.text\n",
    "    outPath = outPath\n",
    "    fileName = os.path.split(d)[1]\n",
    "    if os.path.exists(outPath) == False:\n",
    "        os.mkdir(outPath, mode=0o777,)\n",
    "    outFile = os.path.join(outPath, fileName + outExtension)\n",
    "    open(outFile, \"w\", encoding=\"utf-8\").write(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dictionary/dictionary.json', 'r') as dictionary_file:\n",
    "    dictionary = json.load(dictionary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_index_array(text):\n",
    "    words = kpt.text_to_word_sequence(text)\n",
    "    wordIndices = []\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            wordIndices.append(dictionary[word])\n",
    "        else:\n",
    "            pass\n",
    "            #print(\"'%s' not in training corpus; ignoring.\" %(word))\n",
    "    return wordIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sentiment(tweet_text):\n",
    "    labels = ['positive', 'negative']\n",
    "    testArr = convert_text_to_index_array(tweet_text)\n",
    "    twt = tokenizer.sequences_to_matrix([testArr], mode='binary')\n",
    "    twt = pad_sequences(twt, maxlen=86, dtype='int32', padding='post', truncating='post', value=0)\n",
    "    sentiment = model.predict(twt)\n",
    "    accuracy = sentiment[0][np.argmax(sentiment)] * 100\n",
    "    tweetSent = labels[np.argmax(sentiment)]\n",
    "    return tweetSent, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentiment(object_id, text_to_analyze):\n",
    "    sent_sp = TextBlob(text_to_analyze)\n",
    "    subjectivity = sent_sp.sentiment.subjectivity\n",
    "    polarity = sent_sp.sentiment.polarity\n",
    "    sentiment_tf = Sentiment(text_to_analyze)\n",
    "    classification_tf = sentiment_tf[0]\n",
    "    if classification_tf == 'positive':\n",
    "        classification_num = 1\n",
    "    else:\n",
    "        classification_num = 0\n",
    "    accuracy_tf = sentiment_tf[1]\n",
    "    text_sentiment = [object_id, subjectivity, polarity, classification_tf, accuracy_tf, classification_num]\n",
    "    return text_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_path)\n",
    "df.dropna()\n",
    "text_df = df[['OBJECTID', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>dtg</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>39.274819</td>\n",
       "      <td>-76.608696</td>\n",
       "      <td>Mon Apr 27 23:00:57 +0000 2015</td>\n",
       "      <td>PandaMc8</td>\n",
       "      <td>3.880081e+08</td>\n",
       "      <td>WTFFFFFF https://t.co/2DT5PxqOc2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>39.292146</td>\n",
       "      <td>-76.567825</td>\n",
       "      <td>Mon Apr 27 23:01:15 +0000 2015</td>\n",
       "      <td>okaykerra</td>\n",
       "      <td>3.519059e+08</td>\n",
       "      <td>Pretty Rick been everywhere and ain't been ain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>39.293876</td>\n",
       "      <td>-76.682365</td>\n",
       "      <td>Mon Apr 27 23:01:41 +0000 2015</td>\n",
       "      <td>letgoletkarma</td>\n",
       "      <td>4.498094e+07</td>\n",
       "      <td>I'm filing exempt tomorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>39.309108</td>\n",
       "      <td>-76.666054</td>\n",
       "      <td>Mon Apr 27 23:01:47 +0000 2015</td>\n",
       "      <td>PrettyMoee</td>\n",
       "      <td>2.656314e+08</td>\n",
       "      <td>I got endless videos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>39.281066</td>\n",
       "      <td>-76.631622</td>\n",
       "      <td>Mon Apr 27 23:02:05 +0000 2015</td>\n",
       "      <td>khyona_</td>\n",
       "      <td>2.157380e+09</td>\n",
       "      <td>Omg they mace the man</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  OBJECTID        lat       long                             dtg  \\\n",
       "0        1  39.274819 -76.608696  Mon Apr 27 23:00:57 +0000 2015   \n",
       "1        2  39.292146 -76.567825  Mon Apr 27 23:01:15 +0000 2015   \n",
       "2        3  39.293876 -76.682365  Mon Apr 27 23:01:41 +0000 2015   \n",
       "3        4  39.309108 -76.666054  Mon Apr 27 23:01:47 +0000 2015   \n",
       "4        5  39.281066 -76.631622  Mon Apr 27 23:02:05 +0000 2015   \n",
       "\n",
       "       user_name       user_id  \\\n",
       "0       PandaMc8  3.880081e+08   \n",
       "1      okaykerra  3.519059e+08   \n",
       "2  letgoletkarma  4.498094e+07   \n",
       "3     PrettyMoee  2.656314e+08   \n",
       "4        khyona_  2.157380e+09   \n",
       "\n",
       "                                                text  \n",
       "0                   WTFFFFFF https://t.co/2DT5PxqOc2  \n",
       "1  Pretty Rick been everywhere and ain't been ain...  \n",
       "2                         I'm filing exempt tomorrow  \n",
       "3                               I got endless videos  \n",
       "4                             Omg they mace the man   "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_list =[]\n",
    "\n",
    "start_time = time.time()\n",
    "print(start_time)\n",
    "\n",
    "for row in text_df.iterrows():\n",
    "    oid = row[1]['oid']\n",
    "    text = row[1]['text']\n",
    "    if text != None:\n",
    "        try:\n",
    "            classified_text = calculate_sentiment(oid, text)\n",
    "            sentiment_list.append(classified_text)\n",
    "            #print(classified_text)\n",
    "        except:\n",
    "            print(\"Error on oid \" + str(oid))\n",
    "    \n",
    "end_time = time.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_columns = ['oid', 'Subjectivity', 'Polarity', 'Sentiment', 'Accuracy', 'Classification']\n",
    "sentiment_df = pd.DataFrame(sentiment_list, columns=sentiment_columns)\n",
    "sentiment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df, sentiment_df, on='oid')\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "mentions = []\n",
    "\n",
    "for row in merged_df.iterrows():\n",
    "    oid = row[1]['oid']\n",
    "    text = row[1]['text']\n",
    "    user = row[1]['user_name']\n",
    "    match = re.findall(r'@(?i)[a-z0-9_]+', text)\n",
    "    if len(match) > 0:\n",
    "        for handle in match:\n",
    "            mentions.append([\"@\" + user, handle])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_columns = ['User', 'Mentioned']\n",
    "mentions_df = pd.DataFrame(mentions, columns=mentions_columns)\n",
    "mentions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('output/SentimentData.csv')\n",
    "mentions_df.to_csv('output/TwitterMentions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gis = GIS(\"https://esridistributor.maps.arcgis.com\", \"james_jones_ngse\")\n",
    "map = gis.map(\"Baltimore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcgis.features import SpatialDataFrame\n",
    "\n",
    "spatial_df = SpatialDataFrame.from_xy(merged_df, 'lon', 'lat', sr=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(spatial_df.Polarity, merged_df.Subjectivity, s=100, c=merged_df.Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhoods = SpatialDataFrame.from_featureclass(filename = r\"C:\\Users\\jame9353\\Documents\\GitHub\\NLP-Engine-Integration\\Demo Data\\Baltimore_neighborhoods\\nhood_2010.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_fc = gis.content.get('8dc29960489a4d4ea98972e3d0c53e4d')\n",
    "neighborhoods_fc = gis.content.get('225e71a84e554d319224baae9708e1ee')\n",
    "map.add_layer(neighborhoods_fc)\n",
    "map.add_layer(tweets_fc)\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcgis\n",
    "\n",
    "aggregrate_neighborhoods = arcgis.features.analysis.aggregate_points(tweets_fc, neighborhoods_fc, \n",
    "                                                                     keep_boundaries_with_no_points=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggr_neigh_df = SpatialDataFrame.from_layer(aggregrate_neighborhoods)\n",
    "aggr_neigh_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map1 = gis.map(\"Baltimore\")\n",
    "map1.add_layer(aggregrate_neighborhoods, { \"renderer\": \"ClassedSizeRenderer\", \"field_name\":\"Count\"})\n",
    "map1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_spots = arcgis.features.analyze_patterns.find_hot_spots(tweets_fc, analysis_field='Polarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map2 = gis.map(\"Baltimore\")\n",
    "map2.add_layer(aggregrate_neighborhoods)\n",
    "map2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = TextBlob(text, analyzer=NaiveBayesAnalyzer())\n",
    "classification_nba = sa.sentiment[0]\n",
    "p_pos = sa.sentiment[1]\n",
    "p_neg = sa.sentiment[2]\n",
    "\n",
    "extractor = ConllExtractor()\n",
    "np_extract = TextBlob(text, np_extractor=extractor)\n",
    "np_list = [name for name in np_extract]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    oid = row[1]['oid']\n",
    "    lat = row[1]['lat']\n",
    "    lon = row[1]['lon']\n",
    "    date = row[1]['dtg']\n",
    "    user = row[1]['user_name']\n",
    "    text = row[1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
